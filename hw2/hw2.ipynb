{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, что вам нужно подготовить аналитический отчет по этим отзывам — например, для производителя нового продукта этой категории. Для этого будем искать упоминания товаров в отзывах (будем считать их NE). Учтите, что упоминание может выглядеть не только как \"Iphone 10\", но и как \"модель\", \"телефон\" и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "from nltk.collocations import *\n",
    "from pprint import pprint\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные. Я взяла датасет 5-core pet supplies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse('Pet_Supplies.json.gz'):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "      <td>01 12, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "      <td>09 14, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2CR37UY3VR7BN</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Michelle Ashbery</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought the triliogy and have tested out all ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Entertaining</td>\n",
       "      <td>1355875200</td>\n",
       "      <td>12 19, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2A4COGL9VW2HY</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Michelle P</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>My female kitty could care less about these vi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Happy to have them</td>\n",
       "      <td>1305158400</td>\n",
       "      <td>05 12, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2UBQA85NIGLHA</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Tim  Isenhour \"Timbo\"</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>If I had gotten just volume two, I would have ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>You really only need vol 2</td>\n",
       "      <td>1330905600</td>\n",
       "      <td>03 5, 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin           reviewerName helpful  \\\n",
       "0  A14CK12J7C7JRK  1223000893     Consumer in NorCal  [0, 0]   \n",
       "1  A39QHP5WLON5HV  1223000893        Melodee Placial  [0, 0]   \n",
       "2  A2CR37UY3VR7BN  1223000893       Michelle Ashbery  [0, 0]   \n",
       "3  A2A4COGL9VW2HY  1223000893             Michelle P  [2, 2]   \n",
       "4  A2UBQA85NIGLHA  1223000893  Tim  Isenhour \"Timbo\"  [6, 7]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...      3.0   \n",
       "1  There are usually one or more of my cats watch...      5.0   \n",
       "2  I bought the triliogy and have tested out all ...      4.0   \n",
       "3  My female kitty could care less about these vi...      4.0   \n",
       "4  If I had gotten just volume two, I would have ...      3.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400   \n",
       "1                           Entertaining for my cats      1379116800   \n",
       "2                                       Entertaining      1355875200   \n",
       "3                                 Happy to have them      1305158400   \n",
       "4                         You really only need vol 2      1330905600   \n",
       "\n",
       "    reviewTime  \n",
       "0  01 12, 2011  \n",
       "1  09 14, 2013  \n",
       "2  12 19, 2012  \n",
       "3  05 12, 2011  \n",
       "4   03 5, 2012  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157836, 8510)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df.asin.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am raising my third Golden Retriever, and this time, decided to try this product -- had it handy just as my puppy arrived.  In the past, I had used a traditional crate for night, and a cage during the day.  This pen is just great with the 8 panels, as you can start with few panels, and add them as the puppy grows.  Changing the configuration takes just seconds.  This product is very well designed and constructed.  My puppy is now 5 months old, and at 45 pounds, this pen still works well.  While she has more freedom as she is more responsible, the pen is still handy for time outs, when you can't supervise her, and now at night I put this pen around her new dog bed -- to contain her in our bedroom at night.  It's fantastic.  I have piece of mind that she is safe, and she likes it -- feels secure.  I do recommend that as the puppy is being house trained, use fewer panels, to discourage accidents -- I learned that one the hard way.  This product will make your life and your puppy's life much easier.  It's always a hard slog with a puppy, but at 5 months life is good, and this product helped a lot.\""
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText[43273]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет содержит 157 836 отзывов на 8 510 товаров. Как видно из случайного отзыва, покупатель называет товар по-разному: this product, this pen, the pen и просто it. При этом название playpen, указанное на [амазоне](https://www.amazon.com/IRIS-Exercise-8-Panel-Playpen-Frosty/dp/B000FS4OYA), не употреблено вообще. Извлекать упоминания из таких данных может быть непросто"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (3 балла)** Предложите 3 способа найти упоминания товаров в отзывах. Например, использовать bootstrapping: составить шаблоны вида \"холодильник XXX\", найти все соответствующие n-граммы и выделить из них называние товара. Могут помочь заголовки и дополнительные данные с Amazon. Какие данные необходимы для каждого из способов? Какие есть достоинства/недостатки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самый очевидный и простой способ: скачать к основному датасету ещё и метаданные, взять оттуда название товара, а из названия извлечь существительные\n",
    "\n",
    "Данные: сами отзывы и заголовки товаров  \n",
    "\n",
    "Достоинства: просто, быстро и никаких дополнительных данных  \n",
    "\n",
    "Недостатки: очень низкий recall, например, из приведённого выше отзыва про собачий манеж не извлеклось бы ничего, потому что автор ни разу не называет товар именно playpen\n",
    "\n",
    "\n",
    "2. Для первого способа расширить возможные варианты названий товара. Во-первых, добавить общие слова типа product, item, purchase. Во-вторых, добавить синонимы и в идеале гиперонимы для слов, извлечённых из названия товара. Гиперонимы удобно доставать через WordNet, но размер словаря там меньше, чем в какой-нибудь большой векторной модели. К тому же, в векторных моделях среди соседей слова тоже могут попадать гиперонимы.\n",
    "\n",
    "Данные: отзывы и названия + WordNet или векторная модель\n",
    "\n",
    "Достоинства: гораздо выше recall за счёт расширения списка слов, которыми может быть назван товар\n",
    "\n",
    "Недостатки: не для всех слов получится собрать синонимы/гиперонимы, особенно если использовать WordNet. Не ясно, сколько слов надо собрать, где поставить порог. Всё ещё не решается проблема с сокращениями типа playpen - pen\n",
    "\n",
    "3. Альтернативный вариант для второго способа: собирать синонимы не из внешних источников, а из самих отзывов. Например, отсортировать все существительные из отзывов по косинусной близости к существительным из названий товаров и к общему слову типа product.\n",
    "\n",
    "Данные: отзывы и названия + векторная модель\n",
    "\n",
    "Достоинства: полученные синонимы будут более полезными, потому что они все гарантированно будут содержаться в наших отзывах\n",
    "\n",
    "Недостатки: всё ещё не понятно, где ставить порог, какая близость будет считаться достаточной. Не все слова будут в модели\n",
    "\n",
    "4. Ещё один альтернативный вариант, решающий проблему с порогом: использовать кластеризацию. Не вручную задавать порог, а автоматически кластеризовать существительные из отзывов и брать тот кластер или кластеры, куда вошли изначальные слова из названий и слово product.\n",
    "\n",
    "Данные: отзывы и названия + векторная модель\n",
    "\n",
    "Достоинства: все синонимы гарантированно будут в отзывах. Не надо мучиться с определением порога близости или необходимого числа синонимов\n",
    "\n",
    "Недостатки: я придумала этот способ совершенно из головы, пытаясь представить, каким способом я сама, читая отзывы, пыталась бы понять, о каком товаре они написаны. Очень вероятно, что результат будет не такой классный, какой мне представляется в воображении. Надо пробовать (и playpen - pen тоже вряд ли извлечётся, но это решить кажется нереально на уровне слов, а не символов)\n",
    "\n",
    "(5. GPT-3. Меня ужасно вдохновляет идея про few-shot и one-shot learning. Надеюсь, когда-нибудь почти любая текстовая задача будет хорошо решаться на основе пары ручных примеров)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (2 балла)** Реализуйте один из предложенных вами способов.  \n",
    "\n",
    "Я попробую сделать 4 способ. Для кластеризации использую метод иерархической кластеризации, т. к. он не требует задавать количество кластеров.\n",
    "\n",
    "Туду:\n",
    "* ✔ связать asin на текст названия\n",
    "* ✔ распарсить тексты и достать все существительные (к сущ. из названия добавить product)\n",
    "* ✔ сгруппировать отзывы по товарам и дальше идти отдельно по группам\n",
    "* ✔ для каждого сущ достать вектора из модели\n",
    "* ✔ кластеризовать результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собрать нужные метаданные в датафрейм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703\n",
      "{'title': 'Cat Sitter DVD Trilogy - Vol 1, Vol 2 and Vol 3', 'asin': '1223000893'}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with gzip.open('meta_Pet_Supplies.json.gz') as f:\n",
    "    for l in f:\n",
    "        d = json.loads(l.strip())\n",
    "        if d['asin'] in idxs:\n",
    "            data.append({'title': d['title'],\n",
    "                         'asin': d['asin']})\n",
    "    \n",
    "# total length of list, this number equals total number of products\n",
    "print(len(data))\n",
    "\n",
    "# first row of the list\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat Sitter DVD Trilogy - Vol 1, Vol 2 and Vol 3</td>\n",
       "      <td>1223000893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LitterMaid LM900 Mega Self-Cleaning Litter Box</td>\n",
       "      <td>B00005MF9U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LitterMaid Universal Cat Privacy Tent (LMT100)</td>\n",
       "      <td>B00005MF9V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LitterMaid LM500 Automated Litter Box</td>\n",
       "      <td>B00005MF9T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LitterMaid Waste Receptacles Automatic Litter ...</td>\n",
       "      <td>B00005MF9W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        asin\n",
       "0    Cat Sitter DVD Trilogy - Vol 1, Vol 2 and Vol 3  1223000893\n",
       "1     LitterMaid LM900 Mega Self-Cleaning Litter Box  B00005MF9U\n",
       "2     LitterMaid Universal Cat Privacy Tent (LMT100)  B00005MF9V\n",
       "3              LitterMaid LM500 Automated Litter Box  B00005MF9T\n",
       "4  LitterMaid Waste Receptacles Automatic Litter ...  B00005MF9W"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.DataFrame(data)\n",
    "df_meta.drop_duplicates(inplace=True)\n",
    "df_meta['title'] = df_meta.title.str.replace('&amp;', '&')\n",
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достать и сохранить леммы существительных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(df_col):\n",
    "    nouns = []\n",
    "    for doc in nlp.pipe(iter(df_col.str.lower())):\n",
    "        n = [token.lemma_ for token in doc if str(token).isalpha() and token.pos_ == 'NOUN']\n",
    "        nouns.append(list(set(n)))\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>asin</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat Sitter DVD Trilogy - Vol 1, Vol 2 and Vol 3</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>[product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LitterMaid LM900 Mega Self-Cleaning Litter Box</td>\n",
       "      <td>B00005MF9U</td>\n",
       "      <td>[self, box, litter, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LitterMaid Universal Cat Privacy Tent (LMT100)</td>\n",
       "      <td>B00005MF9V</td>\n",
       "      <td>[privacy, tent, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LitterMaid LM500 Automated Litter Box</td>\n",
       "      <td>B00005MF9T</td>\n",
       "      <td>[product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LitterMaid Waste Receptacles Automatic Litter ...</td>\n",
       "      <td>B00005MF9W</td>\n",
       "      <td>[pack, box, waste, litter, product]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        asin  \\\n",
       "0    Cat Sitter DVD Trilogy - Vol 1, Vol 2 and Vol 3  1223000893   \n",
       "1     LitterMaid LM900 Mega Self-Cleaning Litter Box  B00005MF9U   \n",
       "2     LitterMaid Universal Cat Privacy Tent (LMT100)  B00005MF9V   \n",
       "3              LitterMaid LM500 Automated Litter Box  B00005MF9T   \n",
       "4  LitterMaid Waste Receptacles Automatic Litter ...  B00005MF9W   \n",
       "\n",
       "                                 nouns  \n",
       "0                            [product]  \n",
       "1         [self, box, litter, product]  \n",
       "2             [privacy, tent, product]  \n",
       "3                            [product]  \n",
       "4  [pack, box, waste, litter, product]  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta['nouns'] = get_nouns(df_meta['title'])\n",
    "# учитывать слово product как означающее целевой товар\n",
    "df_meta['nouns'].apply(lambda x: x.append('product'))\n",
    "\n",
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = df_meta.asin.tolist()\n",
    "\n",
    "df.drop(df[~df['asin'].isin(idxs)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94443"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nouns'] = get_nouns(df['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "      <td>01 12, 2011</td>\n",
       "      <td>[picture, tree, cat, wildlife, neighbor, yr, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "      <td>09 14, 2013</td>\n",
       "      <td>[trouble, tv, cat, dvd, mouse, time, bird]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2CR37UY3VR7BN</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Michelle Ashbery</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought the triliogy and have tested out all ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Entertaining</td>\n",
       "      <td>1355875200</td>\n",
       "      <td>12 19, 2012</td>\n",
       "      <td>[one, tv, dvds, cat, sound, triliogy, volume, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2A4COGL9VW2HY</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Michelle P</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>My female kitty could care less about these vi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Happy to have them</td>\n",
       "      <td>1305158400</td>\n",
       "      <td>05 12, 2011</td>\n",
       "      <td>[bit, ape, kitty, video, male]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2UBQA85NIGLHA</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Tim  Isenhour \"Timbo\"</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>If I had gotten just volume two, I would have ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>You really only need vol 2</td>\n",
       "      <td>1330905600</td>\n",
       "      <td>03 5, 2012</td>\n",
       "      <td>[guinea, hand, vol, eye, bird, quality, fisher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin           reviewerName helpful  \\\n",
       "0  A14CK12J7C7JRK  1223000893     Consumer in NorCal  [0, 0]   \n",
       "1  A39QHP5WLON5HV  1223000893        Melodee Placial  [0, 0]   \n",
       "2  A2CR37UY3VR7BN  1223000893       Michelle Ashbery  [0, 0]   \n",
       "3  A2A4COGL9VW2HY  1223000893             Michelle P  [2, 2]   \n",
       "4  A2UBQA85NIGLHA  1223000893  Tim  Isenhour \"Timbo\"  [6, 7]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...      3.0   \n",
       "1  There are usually one or more of my cats watch...      5.0   \n",
       "2  I bought the triliogy and have tested out all ...      4.0   \n",
       "3  My female kitty could care less about these vi...      4.0   \n",
       "4  If I had gotten just volume two, I would have ...      3.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400   \n",
       "1                           Entertaining for my cats      1379116800   \n",
       "2                                       Entertaining      1355875200   \n",
       "3                                 Happy to have them      1305158400   \n",
       "4                         You really only need vol 2      1330905600   \n",
       "\n",
       "    reviewTime                                              nouns  \n",
       "0  01 12, 2011  [picture, tree, cat, wildlife, neighbor, yr, g...  \n",
       "1  09 14, 2013         [trouble, tv, cat, dvd, mouse, time, bird]  \n",
       "2  12 19, 2012  [one, tv, dvds, cat, sound, triliogy, volume, ...  \n",
       "3  05 12, 2011                     [bit, ape, kitty, video, male]  \n",
       "4   03 5, 2012  [guinea, hand, vol, eye, bird, quality, fisher...  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих датафреймах оставлены только пересекающиеся товары и сохранены существительные из них.\n",
    "\n",
    "\n",
    "Для кластеризации надо собрать матрицу с векторами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 97.8% 246.6/252.1MB downloaded"
     ]
    }
   ],
   "source": [
    "vectors = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_clusters(i_clusters, vocab):\n",
    "    clusters = {}\n",
    "    for word_idx, cluster_idx in enumerate(i_clusters):\n",
    "        if cluster_idx not in clusters:\n",
    "            clusters[cluster_idx] = [vocab[word_idx]]\n",
    "        else:\n",
    "            clusters[cluster_idx].append(vocab[word_idx])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл сразу для всего корпуса, а не для отдельных товаров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собрать вместе все сущ из отзывов\n",
    "rev_nouns = []\n",
    "for n_list in df.nouns:\n",
    "    rev_nouns.extend(n_list)\n",
    "# собрать слова из названий\n",
    "title_nouns = []\n",
    "for n_list in df_meta.nouns:\n",
    "    title_nouns.extend(n_list)\n",
    "# добавить слова из названий к словам из отзывов\n",
    "rev_nouns.extend(title_nouns)\n",
    "# оставить по одному разу и только то, что есть в модели\n",
    "rev_nouns = [word for word in set(rev_nouns) if word in vectors]\n",
    "# собрать матрицу векторов\n",
    "arr = np.zeros((len(rev_nouns), 200))\n",
    "for i in range(len(arr)):\n",
    "    arr[i] = vectors[rev_nouns[i]]\n",
    "# кластеризовать\n",
    "i_clusters = hcluster.fclusterdata(arr, 1)\n",
    "# собрать кластеры в читаемый словарь\n",
    "clusters = collect_clusters(i_clusters, rev_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n",
      "running\n",
      "['running', 'run']\n",
      "fleece\n",
      "['fleece']\n",
      "seal\n",
      "['seal', 'sealing']\n",
      "sweater\n",
      "['cashmere', 'turtleneck', 'sweater', 'cardigan']\n",
      "burger\n",
      "['mcdonalds', 'taco', 'hamburger', 'burger']\n",
      "cider\n",
      "['cider']\n",
      "choke\n",
      "['choke']\n",
      "dna\n",
      "['dna']\n",
      "ecosystem\n",
      "['wetland', 'ecosystem', 'habitat']\n",
      "technology\n",
      "['tech', 'technology']\n",
      "chase\n",
      "['chase']\n",
      "silencer\n",
      "['silencer']\n",
      "gripper\n",
      "['gripper']\n",
      "purpose\n",
      "['purpose']\n",
      "goose\n",
      "['goose']\n",
      "hiking\n",
      "['campground', 'camping', 'biking', 'hiking', 'campsite', 'jogging', 'backpacking']\n",
      "cardio\n",
      "['cardio']\n",
      "advantage\n",
      "['advantage']\n",
      "trax\n",
      "['trax']\n",
      "biocube\n"
     ]
    }
   ],
   "source": [
    "# посмотреть что получается\n",
    "print(len(set(title_nouns)))\n",
    "for noun in sample(list(set(title_nouns)), 20):\n",
    "    print(noun)\n",
    "    if noun in rev_nouns:\n",
    "        print(clusters[i_clusters[rev_nouns.index(noun)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15537\n"
     ]
    }
   ],
   "source": [
    "print(len(i_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pups',\n",
       " 'newfoundland',\n",
       " 'puppy',\n",
       " 'brunswick',\n",
       " 'kitten',\n",
       " 'kittens',\n",
       " 'dog',\n",
       " 'pet',\n",
       " 'retriever',\n",
       " 'labrador',\n",
       " 'cats',\n",
       " 'cat']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[i_clusters[rev_nouns.index('dog')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (1 балл)** Соберите n-граммы с полученными сущностями (NE + левый сосед / NE + правый сосед)\n",
    "\n",
    "Туду:\n",
    "* ✔ собрать один список существительных и их синонимов из названий\n",
    "* ✔ лемматизировать отзывы\n",
    "* ✔ достать из лемм-х отзывов соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5936"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "for noun in set(title_nouns):\n",
    "    if noun in rev_nouns: # там то, что было в век. модели\n",
    "        entities.append(noun)\n",
    "        entities.extend(clusters[i_clusters[rev_nouns.index(noun)]])\n",
    "        \n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df_col):\n",
    "    lemmas = []\n",
    "    for doc in nlp.pipe(iter(df_col.str.lower())):\n",
    "        ls = [token.lemma_ for token in doc if str(token).isalpha()]\n",
    "        lemmas.append(ls)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatizedText'] = lemmatize(df['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>nouns</th>\n",
       "      <th>lemmatizedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "      <td>01 12, 2011</td>\n",
       "      <td>[picture, tree, cat, wildlife, neighbor, yr, g...</td>\n",
       "      <td>[i, purchase, the, trilogy, with, hope, -PRON-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>1223000893</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "      <td>09 14, 2013</td>\n",
       "      <td>[trouble, tv, cat, dvd, mouse, time, bird]</td>\n",
       "      <td>[there, be, usually, one, or, more, of, -PRON-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin        reviewerName helpful  \\\n",
       "0  A14CK12J7C7JRK  1223000893  Consumer in NorCal  [0, 0]   \n",
       "1  A39QHP5WLON5HV  1223000893     Melodee Placial  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...      3.0   \n",
       "1  There are usually one or more of my cats watch...      5.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400   \n",
       "1                           Entertaining for my cats      1379116800   \n",
       "\n",
       "    reviewTime                                              nouns  \\\n",
       "0  01 12, 2011  [picture, tree, cat, wildlife, neighbor, yr, g...   \n",
       "1  09 14, 2013         [trouble, tv, cat, dvd, mouse, time, bird]   \n",
       "\n",
       "                                      lemmatizedText  \n",
       "0  [i, purchase, the, trilogy, with, hope, -PRON-...  \n",
       "1  [there, be, usually, one, or, more, of, -PRON-...  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for text in df['lemmatizedText'].tolist():\n",
    "    for i, word in enumerate(text):\n",
    "        if word in entities:\n",
    "            if i != 0:\n",
    "                pairs.append((text[i-1], word))\n",
    "            if i != len(text)-1:\n",
    "                pairs.append((word, text[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4532798"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список пар получился слишком длинным, поэтому, чтобы упростить дальнейшие вычисления, я удалю пары, которые точно не будут верными, и дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [pair for pair in pairs if 'the' not in pair and 'is' not in pair and '-PRON-' not in pair]\n",
    "pairs = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429109"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 'for'), ('guard', 'hair'), ('filter', 'uv'), ('apple', 'blossom'), ('well', 'right')]\n"
     ]
    }
   ],
   "source": [
    "print(list(pairs)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гораздо лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (3 балла)** Ранжируйте n-граммы с помощью 3 коллокационных метрик (t-score, PMI и т.д.). Не забудьте про частотный фильтр / сглаживание. Выберите лучший результат (какая метрика ранжирует выше коллокации, подходящие для отчёта)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words([word for l_list in df['lemmatizedText'].tolist() for word in l_list])\n",
    "finder.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_pmi = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129046"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bigrams_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди всех биграмм отобрать нужные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(all_bigrams):\n",
    "    bigrams_only = list(map(lambda x: x[0], all_bigrams))\n",
    "    picked = [pair for pair in bigrams_only if pair in pairs]\n",
    "    return picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_pmi = pick(all_bigrams_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('playbird', 'mansion'),\n",
      " ('penn', 'plax'),\n",
      " ('luxate', 'patella'),\n",
      " ('miller', 'forge'),\n",
      " ('choy', 'mustard'),\n",
      " ('hypo', 'allergenic'),\n",
      " ('radio', 'shack'),\n",
      " ('hockey', 'puck'),\n",
      " ('jeep', 'wrangler'),\n",
      " ('aspergillus', 'niger'),\n",
      " ('lithium', 'ion'),\n",
      " ('gulf', 'coast'),\n",
      " ('titanium', 'dioxide'),\n",
      " ('jackson', 'galaxy'),\n",
      " ('tum', 'tum')]\n"
     ]
    }
   ],
   "source": [
    "pprint(picked_pmi[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student t**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_t = finder.score_ngrams(bigram_measures.student_t)\n",
    "picked_t = pick(all_bigrams_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 'a'),\n",
      " ('this', 'be'),\n",
      " ('this', 'product'),\n",
      " ('easy', 'to'),\n",
      " ('a', 'little'),\n",
      " ('there', 'be'),\n",
      " ('be', 'very'),\n",
      " ('be', 'not'),\n",
      " ('i', 'can'),\n",
      " ('out', 'of'),\n",
      " ('i', 'buy'),\n",
      " ('a', 'lot'),\n",
      " ('play', 'with'),\n",
      " ('love', 'this'),\n",
      " ('i', 'be')]\n"
     ]
    }
   ],
   "source": [
    "pprint(picked_t[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_like = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "picked_like = pick(all_bigrams_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'product'),\n",
      " ('easy', 'to'),\n",
      " ('play', 'with'),\n",
      " ('be', 'a'),\n",
      " ('a', 'little'),\n",
      " ('litter', 'box'),\n",
      " ('a', 'lot'),\n",
      " ('there', 'be'),\n",
      " ('a', 'bit'),\n",
      " ('lot', 'of'),\n",
      " ('out', 'of'),\n",
      " ('year', 'old'),\n",
      " ('be', 'very'),\n",
      " ('i', 'buy'),\n",
      " ('i', 'can')]\n"
     ]
    }
   ],
   "source": [
    "pprint(picked_like[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во все списки попали пары со словами, которые не являются существительными, и это ошибка пос-теггера spacy (хотя в прошлом году в домашке он показал лучший результат). Чтобы выполнить последний пункт, я возьму метрику pmi, туда попало меньше мусора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (1 балл)** Сгруппируйте полученные коллокации по NE, выведите примеры для 5 товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for pair in picked_pmi:\n",
    "    pair = list(pair)\n",
    "    for i, word in enumerate(pair):\n",
    "        pair.remove(word)\n",
    "        if word in groups:\n",
    "            groups[word].extend(pair)\n",
    "        else:\n",
    "            groups[word] = pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "toy\n",
      "---\n",
      "destroyer\n",
      "poodle\n",
      "fox\n",
      "yorkshire\n",
      "unsupervise\n",
      "\n",
      "treat\n",
      "---\n",
      "motivated\n",
      "maker\n",
      "stuffer\n",
      "dispenser\n",
      "dispense\n",
      "\n",
      "supplement\n",
      "---\n",
      "folic\n",
      "manganous\n",
      "biotin\n",
      "choline\n",
      "niacin\n",
      "\n",
      "leash\n",
      "---\n",
      "attachment\n",
      "handle\n",
      "hook\n",
      "length\n",
      "training\n",
      "\n",
      "harness\n",
      "---\n",
      "vest\n",
      "strap\n",
      "clip\n",
      "instead\n",
      "fit\n"
     ]
    }
   ],
   "source": [
    "examples = ['toy', 'treat', 'supplement', 'leash', 'harness']\n",
    "\n",
    "for word in examples:\n",
    "    print(f'\\n{word}\\n---')\n",
    "    print('\\n'.join(groups[word][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты вышли хуже, чем я ожидала. Возможно, из-за того, что при пос-теггинге просочилось много мусора, я не смогла адекватно оценить метрики на топ-15 коллокаций и выбрала не самую хорошую. Или, возм"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
